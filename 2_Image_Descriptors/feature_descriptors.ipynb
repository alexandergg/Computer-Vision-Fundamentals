{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision – LBP, HOG & Feature Extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un descriptor de características es un algoritmo y una metodología que nos dice cómo se cuantifica localmente una región de entrada de una imagen. Un descriptor de características acepta una única imagen de entrada y devuelve múltiples vectores de características.\n",
    "\n",
    "En la práctica, solo queremos describir, es decir, extraer características de las regiones salientes o \"interesantes\" de una imagen. Es común que apliquemos detectores de keypoint para encontrar regiones \"interesantes\" de una imagen. Estas regiones se usan luego como entrada para nuestros descriptores de características: dado que N regiones interesantes de una imagen deben describirse, recibiremos N vectores de características del descriptor de características. Ejemplos de descriptores de características incluyen SIFT, SURF, ORB, BRISK, BRIEF y FREAK.\n",
    "\n",
    "Los descriptores de características tienden a ser mucho más poderosos que nuestros descriptores de imagen básicos, ya que toman en cuenta la localidad de las regiones en una imagen y las describen por separado. Los descriptores de características también tienden a ser mucho más robustos a los cambios en la imagen de entrada, como rotación, traslación, rotación) y cambios en el punto de vista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Binary Descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature\n",
    "import numpy as np\n",
    "\n",
    "class LocalBinaryPatterns:\n",
    "    def __init__(self, numPoints, radius):\n",
    "        self.numPoints = numPoints\n",
    "        self.radius = radius\n",
    "\n",
    "    def describe(self, image, eps=1e-7):\n",
    "        # Calcular la representación del Local Binary Descriptor de la imagen, y luego\n",
    "        # usar la representación LBP para construir el histograma de patrones\n",
    "        lbp = feature.local_binary_pattern(image, self.numPoints, self.radius, method=\"uniform\")\n",
    "        (hist, _) = np.histogram(lbp.ravel(), bins=range(0, self.numPoints + 3),\n",
    "            range=(0, self.numPoints + 2))\n",
    "\n",
    "        # Normalizar el histograma\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= (hist.sum() + eps)\n",
    "\n",
    "        # Devolver el histograma de patrones binarios locales.\n",
    "        return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1. shirt_02.jpg: 0.0073\n",
      "#2. shirt_03.jpg: 0.0119\n",
      "#3. shirt_04.jpg: 0.0153\n"
     ]
    }
   ],
   "source": [
    "# Inicializaremos el descriptor de Local Binary Descriptor e inicializaremos el diccionario de índice\n",
    "# donde el nombre de archivo de la imagen es la clave y las características son el valor\n",
    "desc = LocalBinaryPatterns(24, 8)\n",
    "index = {}\n",
    "\n",
    "# loop sobre las imagenes de ropa\n",
    "for imagePath in paths.list_images(\"C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/lbp/shirts/\"):\n",
    "    # Cargamos la imagen, la convertimos a escala de grises y la describiremos (Histograms)\n",
    "    image = cv2.imread(imagePath)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    hist = desc.describe(gray)\n",
    "    \n",
    "    # Actualizar el diccionario de índice\n",
    "    filename = imagePath[imagePath.rfind(\"/\") + 1:]\n",
    "    index[filename] = hist\n",
    "\n",
    "# Carga de la imagen de consulta y extraemos Local Binary Descriptor de ella\n",
    "query = cv2.imread(\"C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/lbp/queries/query_01.jpg\")\n",
    "queryFeatures = desc.describe(cv2.cvtColor(query, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "# Mostramos la imagen de consulta e inicializamos el diccionario de resultados.\n",
    "cv2.imshow(\"Query\", query)\n",
    "results = {}\n",
    "\n",
    "# loop sobre el Index\n",
    "for (k, features) in index.items():\n",
    "    # Calcular la distancia chi-cuadrado entre las características actuales y la características\n",
    "    # de la consulta, luego actualizar el diccionario de resultados\n",
    "    d = 0.5 * np.sum(((features - queryFeatures) ** 2) / (features + queryFeatures + 1e-10))\n",
    "    results[k] = d\n",
    "\n",
    "# ordenar los resultados\n",
    "results = sorted([(v, k) for (k, v) in results.items()])[:3]\n",
    "\n",
    "# loop sobre los resultados\n",
    "for (i, (score, filename)) in enumerate(results):\n",
    "    print(\"#%d. %s: %.4f\" % (i + 1, filename, score))\n",
    "    image = cv2.imread(\"C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/lbp/shirts\" + \"/\" + filename)\n",
    "    cv2.imshow(\"Result #{}\".format(i + 1), image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOG (Histogram Oriented Gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El histograma de gradientes orientados o HOG, son descriptores utilizados principalmente en la visión por computadora y el aprendizaje automático para la detección de objetos. Sin embargo, también podemos usar descriptores HOG para cuantificar y representar tanto la forma como la textura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Normalizar la imagen antes de la descripción(Histogramas).\n",
    "2. Calcular gradientes en las direcciones x e y.\n",
    "3. Obtención de votos ponderados en celdas espaciales y de orientación.\n",
    "4. Contrastar que normaliza las celdas espaciales superpuestas.\n",
    "5. Recopilación de todos los histogramas de gradientes orientados para formar el vector de características final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from skimage import exposure\n",
    "from skimage import feature\n",
    "from imutils import paths\n",
    "import imutils\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] extracting features...\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/audi\\audi_01.png\n",
      "audi\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/audi\\audi_02.png\n",
      "audi\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/audi\\audi_03.png\n",
      "audi\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/audi\\audi_04.png\n",
      "audi\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/audi\\audi_05.png\n",
      "audi\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/ford\\ford_01.png\n",
      "ford\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/ford\\ford_02.png\n",
      "ford\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/ford\\ford_03.png\n",
      "ford\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/ford\\ford_04.png\n",
      "ford\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/ford\\ford_05.png\n",
      "ford\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/honda\\honda_01.png\n",
      "honda\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/honda\\honda_02.png\n",
      "honda\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/honda\\honda_03.png\n",
      "honda\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/honda\\honda_04.png\n",
      "honda\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/honda\\honda_05.png\n",
      "honda\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/subaru\\subaru_01.png\n",
      "subaru\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/subaru\\subaru_02.png\n",
      "subaru\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/subaru\\subaru_03.png\n",
      "subaru\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/subaru\\subaru_04.png\n",
      "subaru\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/subaru\\subaru_05.png\n",
      "subaru\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/volkswagen\\volkswagen_01.png\n",
      "volkswagen\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/volkswagen\\volkswagen_02.png\n",
      "volkswagen\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/volkswagen\\volkswagen_03.png\n",
      "volkswagen\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/volkswagen\\volkswagen_04.png\n",
      "volkswagen\n",
      "C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/volkswagen\\volkswagen_05.png\n",
      "volkswagen\n",
      "[INFO] training classifier...\n",
      "[INFO] evaluating...\n"
     ]
    }
   ],
   "source": [
    "# Inicializamos la matriz de datos y de labels.\n",
    "print(\"[INFO] extracting features...\")\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# loop sobre los caminos de imagen en el conjunto de entrenamiento\n",
    "for imagePath in paths.list_images(\"C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/car_logos/\"):\n",
    "    # extraer la marca del coche\n",
    "    print(imagePath)\n",
    "    filename_w_ext = os.path.basename(imagePath)\n",
    "    filename, file_extension = os.path.splitext(filename_w_ext)\n",
    "    filename = filename.split(\"_\")\n",
    "    print(filename[0])\n",
    "\n",
    "    # Cargamos la imagen, y la convertimos a escala de grises y detecte los bordes.\n",
    "    image = cv2.imread(imagePath)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    edged = imutils.auto_canny(gray)\n",
    "\n",
    "    \n",
    "    # Tenemos que encontrar contornos en el \"edge map\", manteniendo solo el más grande, ya que\n",
    "    # se supone que es el logo de la marca\n",
    "    cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[1]\n",
    "    c = max(cnts, key=cv2.contourArea)\n",
    "\n",
    "    # extraer el logo del auto y cambiar su tamaño a un ancho canónico y altura\n",
    "    (x, y, w, h) = cv2.boundingRect(c)\n",
    "    logo = gray[y:y + h, x:x + w]\n",
    "    logo = cv2.resize(logo, (200, 100))\n",
    "\n",
    "    # Extraer el HOG(Histogram Oriented Gradients) del logo.\n",
    "    H = feature.hog(logo, orientations=9, pixels_per_cell=(10, 10),\n",
    "        cells_per_block=(2, 2), transform_sqrt=True, block_norm=\"L1\")\n",
    "\n",
    "    # Actualizar los datos y etiquetas\n",
    "    data.append(H)\n",
    "    labels.append(filename[0])\n",
    "\n",
    "# \"Train\" el clasificador de KNN\n",
    "print(\"[INFO] training classifier...\")\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(data, labels)\n",
    "print(\"[INFO] evaluating...\")\n",
    "\n",
    "# loop sobre el conjunto de datos de prueba\n",
    "for (i, imagePath) in enumerate(paths.list_images(\"C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/hog/test_images\")):\n",
    "    # Cargar la imagen de prueba, convertirla a escala de grises y cambiar su tamaño a\n",
    "    # el tamaño canónico\n",
    "    image = cv2.imread(imagePath)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    logo = cv2.resize(gray, (200, 100))\n",
    "\n",
    "    # Extraer el HOG(Histogram Oriented Gradients) de la imagen de prueba y\n",
    "    # predecir la marca del coche\n",
    "    (H, hogImage) = feature.hog(logo, orientations=9, pixels_per_cell=(10, 10),\n",
    "        cells_per_block=(2, 2), transform_sqrt=True, block_norm=\"L1\", visualize=True)\n",
    "    pred = model.predict(H.reshape(1, -1))[0]\n",
    "\n",
    "    # Visualizar la imgen proveniente del HOG\n",
    "    hogImage = exposure.rescale_intensity(hogImage, out_range=(0, 255))\n",
    "    hogImage = hogImage.astype(\"uint8\")\n",
    "    cv2.imshow(\"HOG Image #{}\".format(i + 1), hogImage)\n",
    "\n",
    "    cv2.putText(image, pred.title(), (10, 35), cv2.FONT_HERSHEY_SIMPLEX, 1.0,\n",
    "        (0, 255, 0), 3)\n",
    "    cv2.imshow(\"Test Image #{}\".format(i + 1), image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extractor and keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DoG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of keypoints: 1082\n"
     ]
    }
   ],
   "source": [
    "# Cargar la imagen y convertirla a escala de grises\n",
    "image = cv2.imread(\"C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/supper.jpg\")\n",
    "orig = image.copy()\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "if imutils.is_cv2():\n",
    "    detector = cv2.FeatureDetector_create(\"SIFT\")\n",
    "    kps = detector.detect(gray)\n",
    "\n",
    "else:\n",
    "    detector = cv2.xfeatures2d.SIFT_create()\n",
    "    (kps, _) = detector.detectAndCompute(gray, None)\n",
    "\n",
    "print(\"# of keypoints: {}\".format(len(kps)))\n",
    "\n",
    "# Loop de los puntos clave y mostrarlos\n",
    "for kp in kps:\n",
    "    r = int(0.5 * kp.size)\n",
    "    (x, y) = np.int0(kp.pt)\n",
    "    cv2.circle(image, (x, y), r, (0, 255, 255), 2)\n",
    "\n",
    "cv2.imshow(\"Images\", np.hstack([orig, image]))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] # of keypoints detected: 1082\n",
      "[INFO] feature vector shape: (1082, 128)\n"
     ]
    }
   ],
   "source": [
    "# Cargar la imagen y convertirla a escala de grises\n",
    "image = cv2.imread(\"C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/supper.jpg\")\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "if imutils.is_cv2():\n",
    "    # Inicializar el detector de \"Keypoints\" y el \"local invariant descriptor\".\n",
    "    detector = cv2.FeatureDetector_create(\"SIFT\")\n",
    "    extractor = cv2.DescriptorExtractor_create(\"SIFT\")\n",
    "\n",
    "    # detectar \"Keypoints\" y luego extraer los \"local invariant descriptor\"\n",
    "    kps = detector.detect(gray)\n",
    "    (kps, descs) = extractor.compute(gray, kps)\n",
    "\n",
    "else:\n",
    "    # Inicializar el detector de \"Keypoints\"\n",
    "    detector = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    # detectar \"Keypoints\" y luego extraer los \"local invariant descriptor\"\n",
    "    (kps, descs) = detector.detectAndCompute(gray, None)\n",
    "\n",
    "# Mostrar el array de \"Keypoints\" y de \"local invariant descriptor\".\n",
    "print(\"[INFO] # of keypoints detected: {}\".format(len(kps)))\n",
    "print(\"[INFO] feature vector shape: {}\".format(descs.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast-Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of keypoints: 2115\n"
     ]
    }
   ],
   "source": [
    "# Cargar la imagen y convertirla a escala de grises\n",
    "image = cv2.imread(\"C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/supper.jpg\")\n",
    "orig = image.copy()\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "if imutils.is_cv2():\n",
    "    detector = cv2.FeatureDetector_create(\"SURF\")\n",
    "    kps = detector.detect(gray)\n",
    "\n",
    "else:\n",
    "    detector = cv2.xfeatures2d.SURF_create()\n",
    "    (kps, _) = detector.detectAndCompute(gray, None)\n",
    "\n",
    "print(\"# of keypoints: {}\".format(len(kps)))\n",
    "\n",
    "# Loop sobre los \"Keypoints\" y mostrarlos\n",
    "for kp in kps:\n",
    "    r = int(0.5 * kp.size)\n",
    "    (x, y) = np.int0(kp.pt)\n",
    "    cv2.circle(image, (x, y), r, (0, 255, 255), 2)\n",
    "\n",
    "cv2.imshow(\"Images\", np.hstack([orig, image]))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SURF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] # of keypoints detected: 2115\n",
      "[INFO] feature vector shape(rows, columns): (2115, 64)\n"
     ]
    }
   ],
   "source": [
    "# Cargar la imagen y convertirla a escala de grises\n",
    "image = cv2.imread(\"C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/supper.jpg\")\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "if imutils.is_cv2():\n",
    "    # Inicializar el detector de \"Keypoints\" y el \"local invariant descriptor\".\n",
    "    detector = cv2.FeatureDetector_create(\"SURF\")\n",
    "    extractor = cv2.DescriptorExtractor_create(\"SURF\")\n",
    "\n",
    "    # detectar \"Keypoints\" y luego extraer los \"local invariant descriptor\"\n",
    "    kps = detector.detect(gray)\n",
    "    (kps, descs) = extractor.compute(gray, kps)\n",
    "\n",
    "else:\n",
    "    # Inicializar el detector de \"Keypoints\"\n",
    "    detector = cv2.xfeatures2d.SURF_create()\n",
    "\n",
    "    # detectar \"Keypoints\" y luego extraer los \"local invariant descriptor\"\n",
    "    (kps, descs) = detector.detectAndCompute(gray, None)\n",
    "\n",
    "# Mostrar el array de \"Keypoints\" y de \"local invariant descriptor\".\n",
    "print(\"[INFO] # of keypoints detected: {}\".format(len(kps)))\n",
    "print(\"[INFO] feature vector shape(rows, columns): {}\".format(descs.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Featured Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "from imutils.feature.factories import FeatureDetector_create, DescriptorExtractor_create, DescriptorMatcher_create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of keypoints from first image: 3077\n",
      "# of keypoints from second image: 1480\n",
      "# of matched keypoints: 17\n"
     ]
    }
   ],
   "source": [
    "detector = \"SURF\"\n",
    "extractor = \"SIFT\"\n",
    "visualize = \"Yes\"\n",
    "\n",
    "# inicializar el detector de características\n",
    "# como \"DOG\" o \"FASTHESSIAN\"\n",
    "if detector == \"DOG\":\n",
    "    detector = FeatureDetector_create(\"SIFT\")\n",
    "elif detector == \"FASTHESSIAN\":\n",
    "    detector = FeatureDetector_create(\"SURF\")\n",
    "else:\n",
    "    detector = FeatureDetector_create(detector)\n",
    "\n",
    "# Inicializar el extractor de características.\n",
    "extractor = DescriptorExtractor_create(extractor)\n",
    "\n",
    "# Inicializar el \"matcher\" de los keypoints\n",
    "matcher = DescriptorMatcher_create(\"BruteForce\")\n",
    "\n",
    "# load the two images and convert them to grayscale\n",
    "imageA = cv2.imread(\"C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/ironman.jpg\")\n",
    "imageB = cv2.imread(\"C:/Users/algonzalez/source/repos/Computer_Vision/2_Image_Descriptors/images/iron-man.jpg\")\n",
    "grayA = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)\n",
    "grayB = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# detectar \"keypoints\" en las dos imágenes\n",
    "kpsA = detector.detect(grayA)\n",
    "kpsB = detector.detect(grayB)\n",
    "\n",
    "# extraer características de cada una de las regiones de \"keypoints\" en las imágenes\n",
    "(kpsA, featuresA) = extractor.compute(grayA, kpsA)\n",
    "(kpsB, featuresB) = extractor.compute(grayB, kpsB)\n",
    "\n",
    "# Hacer que coincidan con los puntos clave o \"keypoints\" utilizando la distancia euclidiana e inicializar\n",
    "# la lista de coincidencias reales\n",
    "rawMatches = matcher.knnMatch(featuresA, featuresB, 2)\n",
    "matches = []\n",
    "\n",
    "if rawMatches is not None:\n",
    "    # Loop sobre los matches en bruto\n",
    "    for m in rawMatches:\n",
    "        # Asegurar que la distancia pase la prueba de relación de David Lowe.\n",
    "        if len(m) == 2 and m[0].distance < m[1].distance * 0.8:\n",
    "            matches.append((m[0].trainIdx, m[0].queryIdx))\n",
    "\n",
    "    # mostrar información del diagnóstico\n",
    "    print(\"# of keypoints from first image: {}\".format(len(kpsA)))\n",
    "    print(\"# of keypoints from second image: {}\".format(len(kpsB)))\n",
    "    print(\"# of matched keypoints: {}\".format(len(matches)))\n",
    "\n",
    "    # Inicializar la imagen de visualización de salida.\n",
    "    (hA, wA) = imageA.shape[:2]\n",
    "    (hB, wB) = imageB.shape[:2]\n",
    "    vis = np.zeros((max(hA, hB), wA + wB, 3), dtype=\"uint8\")\n",
    "    vis[0:hA, 0:wA] = imageA\n",
    "    vis[0:hB, wA:] = imageB\n",
    "\n",
    "    # loop sobre los matches\n",
    "    for (trainIdx, queryIdx) in matches:\n",
    "        # Generar un color al azar y dibuja el \"match\".\n",
    "        color = np.random.randint(0, high=255, size=(3,))\n",
    "        color = tuple(map(int, color))\n",
    "        ptA = (int(kpsA[queryIdx].pt[0]), int(kpsA[queryIdx].pt[1]))\n",
    "        ptB = (int(kpsB[trainIdx].pt[0] + wA), int(kpsB[trainIdx].pt[1]))\n",
    "        cv2.line(vis, ptA, ptB, color, 2)\n",
    "\n",
    "        if visualize == \"Each\":\n",
    "            cv2.imshow(\"Matched\", vis)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "    if visualize == \"Yes\":\n",
    "        cv2.imshow(\"Matched\", vis)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer-vision-env",
   "language": "python",
   "name": "computer-vision-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "616px",
   "left": "0px",
   "right": "20px",
   "top": "106px",
   "width": "213px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
